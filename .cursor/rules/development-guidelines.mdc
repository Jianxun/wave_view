---
description: 
globs: 
alwaysApply: true
---
# Python Project Development Guidelines

These guidelines establish a consistent approach for Python development using test-driven practices across multiple chat sessions. AI agents should reference these guidelines when:

- Planning work for new features or tasks
- Implementing test-driven development
- Maintaining cross-session continuity
- Performing code refactoring
- Troubleshooting failed tests
- Completing tasks and updating project context

This project follows a task-based and test-driven development approach across multiple chat sessions.

## Development Workflow

### 1. Task Selection and Planning
- Start each session by reviewing the `context/todo.md` file
- Select a specific task from the Current Sprint section
- Break down complex tasks into smaller subtasks as needed

### 2. Test-Driven Development Cycle
- Write tests first that define the expected behavior
- Implement the minimum code needed to pass tests
- Run tests to verify functionality
- Refactor while maintaining test coverage

### 3. Focused Feature Development
**CRITICAL RULE**: When making modifications to existing code, follow these principles:
- **Single Feature Focus**: Edit only ONE focused feature at a time
- **Immediate Test Verification**: Run specific test cases for each feature to verify correctness
- **Test-First Development**: If no test case exists for the feature, create one before implementing
- **Atomic Changes**: Complete one feature entirely (code + tests + verification) before moving to the next
- **Fail Fast**: If any test fails, fix it immediately before proceeding to other features
- **Progressive Development**: Build features incrementally rather than making large bulk changes

**Implementation Process**:
1. Identify the specific feature to implement or fix
2. Create or locate the relevant test case
3. Run the test to establish baseline (should fail for new features)
4. Implement the minimal code to make the test pass
5. Run the test again to verify success
6. Only then move to the next feature

### 4. Python-Specific Practices
- Follow PEP 8 style guidelines
- Use type hints to improve code readability and IDE support
- Organize imports alphabetically with standard library first
- Prefer explicit over implicit code
- Document functions and classes with docstrings

## Version Control Guidelines
- Make frequent, atomic commits with descriptive messages
- Use conventional commit format: `type(scope): description`
- For multi-line commit messages, **completely rewrite** `.git_commit_message` file (don't append) and use `git commit -F .git_commit_message`
- **CRITICAL**: Always rewrite `.git_commit_message` entirely - never append to existing content (avoids duplicate commit messages)
- Ensure `.git_commit_message` is in .gitignore to avoid tracking temporary commit files
- Create feature branches for significant changes
- Ensure all tests pass before committing
- Tag releases with semantic versioning (major.minor.patch)

## Testing Organization
- Organize test files as `test_*.py` in the `/tests/` directory
- Use pytest for test execution and assertions
- For complex components, create subdirectories: `/tests/{component}/`
- Save test fixtures in separate files or a fixtures directory

### Incremental Test Development
**CRITICAL RULE**: Always develop tests incrementally, one test case at a time:
- Create ONE test case and run it to ensure it passes
- Only move to the next test case after the current one is verified
- Use fast and incremental iteration instead of bulk edits
- This approach enables easier troubleshooting and maintenance
- If a test fails, fix it immediately before adding more tests

### Testing Strategies
- **Unit Tests**: Test individual functions/methods in isolation
- **Integration Tests**: Test component interactions
- **End-to-End Tests**: Test complete user workflows
- Aim for 90%+ coverage on core modules
- Test both happy path and error conditions
- Use descriptive test names that explain the scenario
- Mock external dependencies to ensure test reliability

## Debugging and Error Handling
- Use systematic debugging: reproduce, isolate, identify root cause, fix, verify
- Add logging statements for complex logic flows
- Write descriptive error messages that guide users toward solutions
- Use appropriate exception types and handle edge cases gracefully
- Document known issues and their workarounds in `context/memory.md`
- Test error conditions as thoroughly as success conditions

## API Design Guidelines
- Design APIs that are intuitive for first-time users
- Provide sensible defaults to minimize required parameters
- Use consistent parameter names across functions
- Support both simple and advanced use cases
- Maintain backward compatibility in minor version updates
- Provide clear examples in docstrings
- Return consistent data types and structures

## Cross-Session Continuity
- Update context files at the end of each session:
  - `context/memory.md`: Record decisions, state changes, and knowledge
  - `context/todo.md`: Update task status and add new tasks
- Begin each new session with context review

## Refactoring Guidelines
- Schedule regular refactoring sessions in the todo list
- Focus on one aspect per refactoring session (e.g., organization, performance)
- Maintain test coverage during refactoring
- Document architectural changes in `context/memory.md`

## When Tests Fail
- Troubleshoot and fix all errors and warnings
- Record challenging bugs and their solutions in `context/memory.md`

## When Tasks Are Completed
1. Update the todo list to mark task completion
2. Update the memory file to reflect current project state
3. Consider adding a new task for refactoring if appropriate
4. Commit changes with a descriptive message using `.git_commit_message` file
5. End the session or move to the next task
